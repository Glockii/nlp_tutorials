{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import itertools\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import random\n",
    "from collections import Counter, defaultdict, deque\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import multinomial\n",
    "from scipy.spatial import distance\n",
    "from IPython.core.display import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import nltk\n",
    "from typing import List, Tuple, Union, Dict\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nTODO:\\n- subsampling\\n- init vectors with TFIDF instead of one-hot\\n-\\n'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO:\n",
    "- subsampling\n",
    "- init vectors with TFIDF instead of one-hot\n",
    "-\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "    print(\"cuda available\")\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "    print(\"cuda not available\")\n",
    "device = torch.device(dev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(\"war.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "data = data.replace('\\n', ' ')\n",
    "data[:50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "# tokenize and lemmatize\n",
    "tokens = tokenizer.tokenize(data)\n",
    "tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "tokens[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(tokens).most_common()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ?!\n",
    "tokens = [re.sub('[^a-zA-Z\\s\\d]+', '', w) for w in tokens if re.sub('[^a-zA-Z\\s\\d]+', '', w) != '']\n",
    "tokens[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Counter(tokens).most_common()[:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_word_to_idx(tokens, min_words: int, max_freq: float):\n",
    "    count = Counter(tokens).most_common()\n",
    "    max_count = count[0][1]\n",
    "    max_words = max_freq * max_count\n",
    "    filtered_words = [c[0] for c in count if min_words < c[1] < max_words]\n",
    "    word2idx = {word: idx+1 for idx, word in enumerate(filtered_words)}\n",
    "    word2idx[\"<unk>\"] = 0\n",
    "    return word2idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "word2idx = gen_word_to_idx(tokens, min_words=10, max_freq=0.9)\n",
    "# word2idx"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get pairs\n",
    "def batch_generator(tokens, window_size=5):\n",
    "    for i in range(0, len(tokens)-window_size):\n",
    "        context = [word2idx[token] if token in word2idx else word2idx[\"<unk>\"] for token in tokens[i: i+window_size]]\n",
    "        # print(context)\n",
    "        central_word = context[window_size//2]\n",
    "        # print(central_word)\n",
    "        context.pop(window_size//2)  # remove central word from context\n",
    "        pairs = [(central_word, context_word, 1) for context_word in context]\n",
    "        # print(pairs)\n",
    "        # TODO: remove from word2idx <unk> and words from context\n",
    "        neg_words = np.random.choice(list(word2idx.values()), window_size-1)\n",
    "        # print(neg_words)\n",
    "        neg_pairs = [(central_word, neg_word, 0) for neg_word in neg_words]\n",
    "        # print(neg_pairs)\n",
    "        batch = pairs + neg_pairs\n",
    "        # print(batch)\n",
    "        yield batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, path: str = \"war_and_peace.txt\",\n",
    "                 debug: bool = False,\n",
    "                 min_words: int = 10,\n",
    "                 max_freq: float = 0.9,\n",
    "                 window_size: int = 5\n",
    "                 ):\n",
    "        self.debug = debug\n",
    "        self.path = path\n",
    "        self.min_words = min_words\n",
    "        self.max_freq = max_freq\n",
    "        self.window_size = window_size\n",
    "        self.data = self.data_loader()\n",
    "        self.tokens = self.preprocessor(self.data)\n",
    "        self.word2idx = self.gen_word_to_idx(self.tokens)\n",
    "\n",
    "    def data_loader(self):\n",
    "        with open(self.path, \"r\") as f:\n",
    "            data = f.read()\n",
    "        data = data.replace('\\n', ' ')\n",
    "        return data\n",
    "\n",
    "    def preprocessor(self, data):\n",
    "        tokens = tokenizer.tokenize(data)\n",
    "        tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "        tokens = [re.sub('[^a-zA-Z\\s\\d]+', '', w) for w in tokens if re.sub('[^a-zA-Z\\s\\d]+', '', w) != '']\n",
    "        return tokens\n",
    "\n",
    "    def gen_word_to_idx(self, tokens):\n",
    "        count = Counter(tokens).most_common()\n",
    "        max_count = count[0][1]\n",
    "        max_words = self.max_freq * max_count\n",
    "        filtered_words = [c[0] for c in count if self.min_words < c[1] < max_words]\n",
    "        word2idx = {word: idx+1 for idx, word in enumerate(filtered_words)}\n",
    "        word2idx[\"<unk>\"] = 0\n",
    "        return word2idx\n",
    "\n",
    "    def batch_generator(self):\n",
    "        if self.debug:\n",
    "            tokens = self.tokens[:1000]\n",
    "        else:\n",
    "            tokens = self.tokens\n",
    "        for i in range(0, len(tokens)-self.window_size):\n",
    "            context = [self.word2idx[token] if token in self.word2idx else self.word2idx[\"<unk>\"]\n",
    "                       for token in tokens[i: i+self.window_size]]\n",
    "            central_word = context[self.window_size//2]\n",
    "            context.pop(self.window_size//2)  # remove central word from context\n",
    "            pairs = [(central_word, context_word, 1) for context_word in context]\n",
    "            # TODO: remove from word2idx <unk> and words from context\n",
    "            neg_words = np.random.choice(list(self.word2idx.values()), self.window_size-1)\n",
    "            neg_pairs = [(central_word, neg_word, 0) for neg_word in neg_words]\n",
    "            batch = pairs + neg_pairs\n",
    "            yield batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "class SkipGramModel(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size: int = 1000,\n",
    "                 embedding_size: int = 100,\n",
    "                 ):\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        if torch.cuda.is_available():\n",
    "            dev = \"cuda:0\"\n",
    "            print(\"cuda available\")\n",
    "        else:\n",
    "            dev = \"cpu\"\n",
    "            print(\"cuda not available\")\n",
    "        self.device = torch.device(dev)\n",
    "        self.u_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.v_embeddings = nn.Embedding(vocab_size, embedding_size)\n",
    "\n",
    "    def forward(self, u, v):\n",
    "        emb_u = self.u_embeddings(u)  # batch_size * emb_dimension\n",
    "        emb_v = self.v_embeddings(v)  # batch_size * emb_dimension\n",
    "        logits = torch.sum(emb_u * emb_v, dim=-1)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 embedding_size: int = 100,\n",
    "                 debug: bool = False,\n",
    "                 path: str = \"war_and_peace.txt\",\n",
    "                 output_path: str = \"output/\",\n",
    "                 min_words: int = 10,\n",
    "                 max_freq: float = 0.9,\n",
    "                 window_size: int = 5,\n",
    "                 num_epochs: int = 2,\n",
    "                 initial_lr: float = 1e-3,\n",
    "                 ):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        # data params\n",
    "        self.path = path\n",
    "        self.output_path = output_path\n",
    "        self.debug = debug\n",
    "        if self.debug:\n",
    "            print(\"debug mode on\")\n",
    "        self.min_words = min_words\n",
    "        self.max_freq = max_freq\n",
    "        self.embedding_size = embedding_size\n",
    "        self.window_size = window_size\n",
    "        self.data = DataHandler(\n",
    "            debug=debug,\n",
    "            path=path,\n",
    "            min_words=min_words,\n",
    "            max_freq=max_freq,\n",
    "            window_size=window_size\n",
    "        )\n",
    "        # model params\n",
    "        self.num_epochs = num_epochs\n",
    "        self.initial_lr = initial_lr\n",
    "        self.model = SkipGramModel(len(self.data.tokens), self.embedding_size)\n",
    "        self.device = self.model.device\n",
    "        self.model.to(self.device)\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.initial_lr)\n",
    "        self.criterion = torch.nn.BCEWithLogitsLoss()\n",
    "        self.losses = []\n",
    "        # embeddings params\n",
    "        self.vocab_size = len(set(self.data.tokens))\n",
    "        self.u_embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "        self.v_embeddings = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(1, self.num_epochs+1):\n",
    "            tmp_loss = []\n",
    "            for batch in tqdm(self.data.batch_generator()):\n",
    "                # get word ids\n",
    "                u, v, target = zip(*batch)\n",
    "                u, v, target = torch.tensor(u, dtype=torch.int64, device=self.device), \\\n",
    "                               torch.tensor(v, dtype=torch.int64, device=self.device), \\\n",
    "                               torch.tensor(target, dtype=torch.float32, device=self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                logits = self.model(u, v)\n",
    "                loss = self.criterion(logits, target)\n",
    "                tmp_loss.append(loss.mean().item())\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            self.losses.append(np.mean(tmp_loss))\n",
    "            print(epoch, np.round(self.losses, 5))\n",
    "        self.save_model()\n",
    "        self.save_vectors()\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save(self.model, self.output_path+\"model.pth\")\n",
    "\n",
    "    def save_vectors(self):\n",
    "        embedding = (self.model.u_embeddings.weight.cpu().data.numpy()\n",
    "                     + self.model.v_embeddings.weight.cpu().data.numpy())/2\n",
    "        idx2word = {idx: word for word, idx in self.data.word2idx.items()}\n",
    "        vectors_file = open(self.output_path+\"word_vectors.txt\", 'w', encoding='utf-8')\n",
    "        vectors_file.write('%d %d\\n' % (len(idx2word), self.embedding_size))\n",
    "\n",
    "        for wid, w in idx2word.items():\n",
    "            e = embedding[wid]\n",
    "            e = ' '.join(map(lambda x: str(x), e))\n",
    "            vectors_file.write('%s %s\\n' % (w, e))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n",
      "1 [3.14692]\n",
      "2 [3.14692 2.55352]\n",
      "3 [3.14692 2.55352 2.28712]\n",
      "4 [3.14692 2.55352 2.28712 2.11767]\n",
      "5 [3.14692 2.55352 2.28712 2.11767 1.98921]\n",
      "CPU times: user 5h 24min 13s, sys: 2h 20min 43s, total: 7h 44min 57s\n",
      "Wall time: 7h 47min 33s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "576606it [1:33:55, 102.32it/s]\n",
      "576606it [1:33:35, 102.68it/s]\n",
      "576606it [1:33:23, 102.90it/s]\n",
      "576606it [1:33:21, 102.95it/s]\n",
      "576606it [1:33:13, 103.09it/s]\n",
      "/home/n-bar/01_mts/env-nutcracker/lib/python3.6/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type SkipGramModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = Word2Vec(\n",
    "    embedding_size=100,\n",
    "    debug=False,  # !!!\n",
    "    path=\"war_and_peace.txt\",\n",
    "    output_path=\"output/\",\n",
    "    min_words=10,\n",
    "    max_freq=0.9,\n",
    "    window_size=5,\n",
    "    num_epochs=5,  # !!!\n",
    "    initial_lr=1e-3,\n",
    "              )\n",
    "w2v.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[3.1469166241086755,\n 2.553516806131806,\n 2.2871161223628347,\n 2.11766518091871,\n 1.9892079724328273]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "(833, 60, 218, 75)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# king - man + woman â‰ˆ queen\n",
    "w2v.data.word2idx['king'], w2v.data.word2idx['man'], w2v.data.word2idx['woman'], w2v.data.word2idx['princess']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "embedding = (w2v.model.u_embeddings.weight.cpu().data.numpy()\n",
    "                     + w2v.model.v_embeddings.weight.cpu().data.numpy())/2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "result_emb = embedding[833] - embedding[60] + embedding[218]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "0.970347199589014"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "distance.cosine(result_emb, embedding[75])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}